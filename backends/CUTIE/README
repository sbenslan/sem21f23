Toolchain for training Ternary Neural Networks and map to TNN-Accelerator core (layerwise)

TNNUtils.py defines PyTorch classes and layers that are compatible with the mapping software
Right now ConvBlock is sure to work, ResNetBlock does not work yet.

TNNNet.py defines the network architecture and training and testing code.
For the allowed Alphabet C = ConvBlock and M = MaxPool2D, the following expressions can be mapped:
[[C][C]*[M]]*

TNNExtract.py extracts the activations, weights, thresholds and pooling information for a given network
to map it to the accelerator architecture layer-wise for efficiency evaluations

TNNMapper.py map the extracted information from the TNNExtract.py code layerwise to accelerator code, i.e. it compiles
it to signals and defines signals to read out the results from memory.

Be sure to set the tnn-accel project path in TNNMapper.py before running it.
This is necessary to determine the signal widths.
A working conda environment requirements.txt is found in condaEnv.txt

Also make sure to create a symbolic path to global_parameters.py in /tnn_accel/stimuli/global_parameters.py